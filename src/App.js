import React, { useState, useEffect } from "react";
import {
	Database,
	FileText,
	Copy,
	Trophy,
	Image,
	BookOpen,
	BarChart3,
	Target,
	Brain,
	Book,
	Microscope,
	Building,
	User,
	Church,
	Accessibility,
	DollarSign,
	AlertTriangle,
	Scale,
	Ruler,
	Github,
	Sparkles,
	TrendingUp,
} from "lucide-react";
import { MathJaxContext } from "better-react-mathjax";
import BiasChart from "./BiasChart";
import biasData from "./data.json";
import TSNEVisualization from "./TSNEVisualization";
import tsneData from "./tsne_data.json";
import BiasDistributionChart from "./BiasDistributionChart";
import "./App.css";

function App() {
	const [activeSection, setActiveSection] = useState("home");
	const [scrollY, setScrollY] = useState(0);
	const [showCitationCopied, setShowCitationCopied] = useState(false);

	const mathJaxConfig = {
		loader: { load: ["[tex]/html"] },
		tex: {
			inlineMath: [
				["$", "$"],
				["\\(", "\\)"],
			],
			displayMath: [
				["$$", "$$"],
				["\\[", "\\]"],
			],
		},
	};

	useEffect(() => {
		const handleScroll = () => setScrollY(window.scrollY);
		window.addEventListener("scroll", handleScroll);
		return () => window.removeEventListener("scroll", handleScroll);
	}, []);

	const scrollToSection = (sectionId) => {
		const element = document.getElementById(sectionId);
		if (element) {
			element.scrollIntoView({ behavior: "smooth" });
			setActiveSection(sectionId);
		}
	};

	const authors = [
		// { name: 'Santhosh G S', affiliation: 1, isEqualContrib: true },
		{
			name: "Santhosh G S",
			affiliation: 1,
			url: "https://santhosh-gnse.github.io/",
		},
		{
			name: "Akshay Govind S",
			affiliation: 1,
			url: "https://tensorteen.github.io/",
		},
		{
			name: "Gokul S Krishnan",
			affiliation: 1,
			url: "https://gsk1692.github.io/",
		},
		{
			name: "Balaraman Ravindran",
			affiliation: 1,
			url: "https://dsai.iitm.ac.in/~ravi/",
		},
		{
			name: "Sriraam Natarajan",
			affiliation: 2,
			url: "https://personal.utdallas.edu/~sriraam.natarajan/",
		},
	];

	const affiliations = [
		"Centre for Responsible AI, IIT Madras, India",
		"University of Texas at Dallas, USA",
	];

	const bibtex = `@misc{s2025indicasadatasetbiasevaluation,
				title={IndiCASA: A Dataset and Bias Evaluation Framework in LLMs Using Contrastive Embedding Similarity in the Indian Context}, 
				author={Santhosh G S and Akshay Govind S and Gokul S Krishnan and Balaraman Ravindran and Sriraam Natarajan},
				year={2025},
				eprint={2510.02742},
				archivePrefix={arXiv},
				primaryClass={cs.CL},
				url={https://arxiv.org/abs/2510.02742}, 
		}`;

	const handleCopyBibtex = () => {
		navigator.clipboard.writeText(bibtex);
		setShowCitationCopied(true);
		setTimeout(() => setShowCitationCopied(false), 2000);
	};

	return (
		<MathJaxContext config={mathJaxConfig}>
			<div className="App">
				{/* Navigation */}
				<nav className={`navbar ${scrollY > 50 ? "scrolled" : ""}`}>
					<div className="nav-container">
						<div className="nav-logo">
							<span className="logo-main">IndiCASA</span>
							<span className="logo-sub">Bias Evaluation Framework</span>
						</div>
						<div className="nav-links">
							<button
								onClick={() => scrollToSection("home")}
								className={activeSection === "home" ? "active" : ""}>
								Home
							</button>
							<button
								onClick={() => scrollToSection("abstract")}
								className={activeSection === "abstract" ? "active" : ""}>
								Abstract
							</button>
							<button
								onClick={() => scrollToSection("methodology")}
								className={activeSection === "methodology" ? "active" : ""}>
								Method
							</button>
							<button
								onClick={() => scrollToSection("results")}
								className={activeSection === "results" ? "active" : ""}>
								Results
							</button>
							<button
								onClick={() => scrollToSection("dataset")}
								className={activeSection === "dataset" ? "active" : ""}>
								Dataset
							</button>
						</div>
					</div>
				</nav>

				{/* Hero Section */}
				<section id="home" className="hero-section">
					<div className="hero-content">
						<h1 className="hero-title">
							<span
								style={{
									background: "linear-gradient(to right, #fbbf24, #f59e0b)",
									WebkitBackgroundClip: "text",
									WebkitTextFillColor: "transparent",
									fontWeight: "800",
								}}>
								IndiCASA
							</span>
							: A Dataset and Bias Evaluation Framework in LLMs Using
							Contrastive Embedding Similarity in the Indian Context
						</h1>

						<p className="hero-subtitle">
							Accepted at{" "}
							<strong>
								AAAI/ACM Conference on AI, Ethics, and Society (AIES) 2025
							</strong>
						</p>

						{/* Authors Section */}
						<div className="authors-section">
							<div className="authors-list">
								{authors.map((author, idx) => (
									<span key={idx} className="author-name">
										{author.url ? (
											<a
												href={author.url}
												target="_blank"
												rel="noopener noreferrer"
												style={{ color: "inherit", textDecoration: "none" }}>
												{author.name}
											</a>
										) : (
											author.name
										)}
										{author.isEqualContrib && (
											<sup className="equal-contrib">*</sup>
										)}
										<sup className="affiliation-number">
											{Array.isArray(author.affiliation)
												? author.affiliation.join(",")
												: author.affiliation}
										</sup>
										{idx < authors.length - 1 && ", "}
									</span>
								))}
							</div>{" "}
							<div className="affiliations-list">
								{affiliations.map((aff, idx) => (
									<span key={idx} className="affiliation">
										<sup>{idx + 1}</sup> {aff}
										{idx < affiliations.length - 1 && " â€¢ "}
									</span>
								))}
							</div>
						</div>

						{/* Action Buttons */}
						<div className="action-buttons">
							<a
								href="https://arxiv.org/abs/2510.02742"
								target="_blank"
								rel="noopener noreferrer"
								className="btn btn-primary">
								<FileText size={20} />
								Paper
							</a>
							<a
								href="https://drive.google.com/file/d/1KU1TsW109gdSqIcysSvR524JKSSQl8in/view?usp=sharing"
								target="_blank"
								rel="noopener noreferrer"
								className="btn btn-secondary">
								<Image size={20} />
								Poster
							</a>
							<a
								href="https://ojs.aaai.org/index.php/AIES/article/view/36605"
								target="_blank"
								rel="noopener noreferrer"
								className="btn btn-secondary">
								<BookOpen size={20} />
								Proceedings
							</a>
							<a
								href="https://github.com/cerai-iitm/IndiCASA"
								target="_blank"
								rel="noopener noreferrer"
								className="btn btn-secondary">
								<Github size={20} />
								Code
							</a>
							<a
								href="https://github.com/cerai-iitm/IndiCASA/tree/main/IndiCASA_dataset"
								target="_blank"
								rel="noopener noreferrer"
								className="btn btn-secondary">
								<Database size={20} />
								Dataset
							</a>
							<button
								onClick={handleCopyBibtex}
								className={`btn btn-secondary ${
									showCitationCopied ? "copy-success" : ""
								}`}>
								<Copy size={20} />
								{showCitationCopied ? "Citation Copied!" : "Copy Citation"}
							</button>
						</div>
					</div>
				</section>

				{/* Abstract Section */}
				<section id="abstract" className="abstract-section">
					<div className="container">
						<div className="section-header">
							<h2>Abstract</h2>
						</div>

						<div className="abstract-content">
							<div className="abstract-text">
								<p>
									Large Language Models (LLMs) have gained significant traction
									across critical domains owing to their impressive contextual
									understanding and generative capabilities. However, their
									increasing deployment in high stakes applications necessitates
									rigorous evaluation of embedded biases, particularly in
									culturally diverse contexts like India where existing
									embedding-based bias assessment methods often fall short in
									capturing nuanced stereotypes.
								</p>
								<p>
									We propose an evaluation framework based on an encoder trained
									using contrastive learning that captures fine-grained bias
									through embedding similarity. We also introduce a novel
									dataset - <strong>IndiCASA</strong> (<strong>Indi</strong>
									Bias-based <strong>C</strong>ontextually <strong>A</strong>
									ligned <strong>S</strong>tereotypes and <strong>A</strong>
									nti-stereotypes) comprising 2,575 human-validated sentences
									spanning five demographic axes: caste, gender, religion,
									disability, and socioeconomic status.
								</p>
								<p>
									Our evaluation of multiple open-weight LLMs reveals that all
									models exhibit some degree of stereotypical bias, with
									disability related biases being notably persistent, and
									religion bias generally lower likely due to global debiasing
									efforts demonstrating the need for fairer model development.
								</p>
							</div>

							<div className="key-contributions">
								<h3>Key Contributions</h3>
								<div className="contributions-grid">
									<div className="contribution-item">
										<Target size={24} className="contrib-icon" />
										<div className="contrib-content">
											<h4>Context-Aware Framework</h4>
											<p>
												First comprehensive bias evaluation framework
												specifically designed for Indian socio-cultural contexts
											</p>
										</div>
									</div>
									<div className="contribution-item">
										<Brain size={24} className="contrib-icon" />
										<div className="contrib-content">
											<h4>Contrastive Learning</h4>
											<p>
												Novel application of contrastive learning for
												fine-grained bias detection through embedding similarity
											</p>
										</div>
									</div>
									<div className="contribution-item">
										<Book size={24} className="contrib-icon" />
										<div className="contrib-content">
											<h4>Curated Dataset</h4>
											<p>
												2,575 human-validated sentences across five critical
												bias dimensions with expert validation
											</p>
										</div>
									</div>
									<div className="contribution-item">
										<Microscope size={24} className="contrib-icon" />
										<div className="contrib-content">
											<h4>Comprehensive Evaluation</h4>
											<p>
												Extensive assessment of 8+ state-of-the-art LLMs
												revealing universal bias presence
											</p>
										</div>
									</div>
								</div>
							</div>
						</div>
					</div>
				</section>

				{/* Methodology Section */}
				<section id="methodology" className="methodology-section">
					<div className="container">
						<div className="section-header">
							<h2>Methodology</h2>
							<p>
								Our three-phase approach to culturally-aware bias evaluation
							</p>
						</div>

						{/* Three Phases */}
						<div className="phases-container">
							<div className="phase-card">
								<div className="phase-header">
									<div className="phase-number">01</div>
									<h3>Dataset Curation</h3>
								</div>
								<div className="phase-content">
									<div className="phase-description">
										<p>
											Construction of IndiCASA dataset through Human-AI
											collaboration, capturing stereotype-anti-stereotype pairs
											across five demographic dimensions with rigorous expert
											validation.
										</p>
										<ul className="phase-features">
											<li>Human-AI collaborative sentence generation</li>
											<li>Language expert validation</li>
											<li>Social scientist review process</li>
											<li>Quality assurance and cultural sensitivity checks</li>
										</ul>
									</div>
									<div className="phase-visual">
										<div className="visual-container">
											<img
												src={
													process.env.PUBLIC_URL +
													"/images/IndiCASA_DatasetPrepProcess.png"
												}
												alt="Dataset preparation process workflow"
												className="phase-diagram"
											/>
											<div className="visual-caption">
												<strong>Figure 1:</strong> Human-AI collaborative
												dataset curation process with expert validation at each
												stage
											</div>
										</div>
									</div>
								</div>
							</div>

							<div className="phase-card">
								<div className="phase-header">
									<div className="phase-number">02</div>
									<h3>Contrastive Training</h3>
								</div>
								<div className="phase-content">
									<div className="phase-description">
										<p>
											Training encoder models using advanced contrastive
											learning objectives (NT-Xent, Pairwise, Triplet Loss) to
											learn meaningful representations that distinguish bias
											patterns.
										</p>
										<ul className="phase-features">
											<li>Multiple contrastive loss functions</li>
											<li>Hyperparameter optimization</li>
											<li>Model-agnostic framework</li>
											<li>Embedding space transformation</li>
										</ul>
									</div>
									<div className="phase-visual">
										<div className="visual-container">
											<img
												src={
													process.env.PUBLIC_URL +
													"/images/Contrastive_Training.svg"
												}
												alt="Contrastive Training Process showing embedding space transformation"
												className="method-diagram"
											/>
											<div className="visual-caption">
												<strong>Figure 2:</strong> Contrastive training
												transforms embedding space to better distinguish
												stereotypes from anti-stereotypes
											</div>
										</div>
									</div>
								</div>
							</div>

							<div className="phase-card phase-card-with-side-image">
								<div className="phase-main-content">
									<div className="phase-header">
										<div className="phase-number">03</div>
										<h3>Bias Evaluation</h3>
									</div>
									<div className="phase-content">
										<div className="phase-description">
											<p>
												Assessment of Large Language Models using the trained
												encoder to compute bias scores through free-text
												generation and embedding similarity analysis.
											</p>
											<p>
												Our bias quantification framework formalizes model
												stereotyping tendencies through a distance-based metric
												between expected and observed probability distributions.
												We compute empirical distributions from model
												generations using masked templates, then classify
												outputs into stereotype and anti-stereotype categories
												using our contrastive encoder. The Stereotype
												Probability measures the likelihood of generating
												stereotypical content, while the Bias Score quantifies
												absolute deviation from ideal unbiased distributions
												(0-100 scale).
											</p>
											<ul className="phase-features phase-features-single">
												<li>Free-text generation analysis</li>
												<li>Embedding similarity scoring</li>
												<li>Comprehensive model comparison</li>
												<li>Bias score computation (0-100 scale)</li>
											</ul>
										</div>
									</div>
								</div>
								<div className="phase-visual phase-visual-extended">
									<div className="visual-container">
										<img
											src={
												process.env.PUBLIC_URL +
												"/images/Bias_Evaluation_Diagram.svg"
											}
											alt="Bias evaluation pipeline"
											className="phase-diagram"
										/>
										<div className="visual-caption">
											<strong>Figure 3:</strong> Bias evaluation pipeline using
											trained encoder for LLM assessment
										</div>
									</div>
								</div>
							</div>
						</div>
						{/* Bias Dimensions */}
						<div className="bias-dimensions">
							<h3>Evaluated Bias Dimensions</h3>
							<p>
								Our framework evaluates five critical demographic dimensions
								specific to Indian socio-cultural contexts
							</p>
							<div className="dimensions-grid">
								<div className="dimension-card">
									<Building size={32} className="dimension-icon" />
									<h4>Caste</h4>
									<p>
										Complex hierarchical social structures unique to Indian
										society, including subtle contextual biases
									</p>
								</div>
								<div className="dimension-card">
									<User size={32} className="dimension-icon" />
									<h4>Gender</h4>
									<p>
										Gender role stereotypes and expectations in Indian cultural
										contexts
									</p>
								</div>
								<div className="dimension-card">
									<Church size={32} className="dimension-icon" />
									<h4>Religion</h4>
									<p>
										Religious identity-based biases and interfaith stereotypes
									</p>
								</div>
								<div className="dimension-card">
									<Accessibility size={32} className="dimension-icon" />
									<h4>Disability</h4>
									<p>
										Biases affecting individuals with disabilities and mental
										health conditions
									</p>
								</div>
								<div className="dimension-card">
									<DollarSign size={32} className="dimension-icon" />
									<h4>Socioeconomic</h4>
									<p>
										Economic status-based stereotypes and class-related
										assumptions
									</p>
								</div>
							</div>
						</div>
					</div>
				</section>

				{/* Results Section */}
				<section id="results" className="results-section">
					<div className="container">
						<div className="section-header">
							<h2>Results & Analysis</h2>
							<p>
								Encoder training outcomes and comprehensive LLM bias evaluation
							</p>
						</div>
						{/* t-SNE Visualization */}
						<div className="tsne-section">
							<div className="section-header">
								<Sparkles size={28} className="section-icon" />
								<h3>Contrastive Training Visualization</h3>
							</div>
							<p className="section-description">
								Interactive t-SNE projections showing clear clustering of
								stereotypes and anti-stereotypes before and after contrastive
								training across different bias categories.
							</p>
							<TSNEVisualization data={tsneData} />
							<div className="visual-caption">
								<strong>Figure 3:</strong> Use the controls above to explore
								different bias types and model states (Bare vs Finetuned).
							</div>
						</div>
						{/* Encoder Training Results */}
						<div className="encoder-training-section">
							<div className="encoder-header">
								<div className="encoder-title-area">
									<Brain size={65} className="encoder-icon" />
									<div>
										<h3>Encoder Training</h3>
										<p className="encoder-subtitle">
											<TrendingUp size={16} className="subtitle-icon" />
											Contrastive Loss Function Comparison
										</p>
									</div>
								</div>
								<p className="encoder-description">
									Evaluating NT-Xent, Triplet, and Pairwise loss functions to
									train our bias detection encoder - the measurement tool for
									assessing stereotypes across Indian socio-cultural contexts.
								</p>
							</div>

							<div className="encoder-content-wrapper">
								<div className="encoder-chart-section">
									<BiasChart data={biasData} />
									<div className="chart-caption">
										<strong>Figure 4:</strong> Comparative performance of
										contrastive loss functions (NT-Xent, Triplet, Pairwise)
										versus bare model in training the encoder. These results
										show how effectively each training approach learns to
										distinguish stereotypes from anti-stereotypes across
										different bias dimensions. Use the controls to switch
										between aggregated and granular views.
									</div>
								</div>

								<div className="encoder-insights">
									<h4 className="insights-title">Key Insights</h4>
									<div className="insight-item">
										<div className="insight-icon">
											<Trophy size={20} />
										</div>
										<div className="insight-content">
											<h5>NT-Xent Loss Excellence</h5>
											<p>
												NT-Xent (Normalized Temperature-scaled Cross Entropy)
												consistently outperforms other loss functions across
												most bias dimensions, making it our preferred choice for
												the final encoder used in LLM evaluation.
											</p>
										</div>
									</div>
									<div className="insight-item">
										<div className="insight-icon">
											<Target size={20} />
										</div>
										<div className="insight-content">
											<h5>Contrastive Learning Impact</h5>
											<p>
												All contrastive training methods show significant
												improvement over the bare model, validating our approach
												of using contrastive learning for bias detection.
											</p>
										</div>
									</div>
									<div className="insight-item">
										<div className="insight-icon">
											<BarChart3 size={20} />
										</div>
										<div className="insight-content">
											<h5>Dimension-Specific Performance</h5>
											<p>
												Different loss functions excel at different bias types -
												NT-Xent performs best on caste and religion, while
												Triplet loss shows strength in disability bias
												detection.
											</p>
										</div>
									</div>
								</div>
							</div>
						</div>{" "}
						{/* LLM Evaluation Results */}
						<div className="llm-evaluation-section">
							<h3>
								<AlertTriangle
									size={24}
									style={{ display: "inline", marginRight: "10px" }}
								/>
								LLM Bias Evaluation: Using Our Framework
							</h3>
							<div className="section-intro">
								<p className="highlight-text">
									<strong>Framework Overview:</strong> Using the NT-Xent trained
									encoder as our measurement tool, we evaluated multiple Large
									Language Models to assess their bias levels across Indian
									socio-cultural contexts.
								</p>
							</div>

							{/* Key Findings */}
							<div className="findings-section">
								<h4>Key Findings from LLM Evaluation</h4>
								<div className="findings-grid">
									<div className="finding-card major-finding">
										<div className="finding-header">
											<AlertTriangle size={24} className="finding-icon" />
											<h4>Universal Bias Presence</h4>
										</div>
										<p>
											All evaluated models exhibit some degree of stereotypical
											bias, regardless of architecture or size, highlighting the
											pervasive nature of bias in current LLMs.
										</p>
									</div>

									<div className="finding-card">
										<div className="finding-header">
											<Accessibility size={24} className="finding-icon" />
											<h4>Persistent Disability Bias</h4>
										</div>
										<p>
											Disability-related biases show consistently high scores
											across models, with several exceeding 30% bias rate.
										</p>
									</div>

									<div className="finding-card">
										<div className="finding-header">
											<Church size={24} className="finding-icon" />
											<h4>Lower Religion Bias</h4>
										</div>
										<p>
											Religious bias scores are relatively lower, likely due to
											global debiasing efforts during model training.
										</p>
									</div>

									<div className="finding-card">
										<div className="finding-header">
											<Scale size={24} className="finding-icon" />
											<h4>Caste-Socioeconomic Correlation</h4>
										</div>
										<p>
											Caste and socioeconomic bias scores show similar patterns,
											reflecting intersected social hierarchies.
										</p>
									</div>

									<div className="finding-card">
										<div className="finding-header">
											<Ruler size={24} className="finding-icon" />
											<h4>Size vs. Fairness</h4>
										</div>
										<p>
											Model size doesn't correlate linearly with bias levels -
											smaller models can be both more and less biased.
										</p>
									</div>

									<div className="finding-card">
										<div className="finding-header">
											<Target size={24} className="finding-icon" />
											<h4>Context Dependency</h4>
										</div>
										<p>
											Same demographic terms show different bias patterns
											depending on contextual usage.
										</p>
									</div>
								</div>
							</div>
						</div>
					</div>
				</section>

				{/* Dataset Section */}
				<section id="dataset" className="dataset-section">
					<div className="container">
						<div className="section-header">
							<h2>IndiCASA Dataset</h2>
							<p>
								A comprehensive resource for bias evaluation in Indian
								socio-cultural contexts
							</p>
						</div>

						<div className="dataset-overview">
							{/* Dataset Statistics and Distribution Side by Side */}
							<div className="dataset-stats-and-chart">
								{/* Dataset Statistics - Left Side */}
								<div className="dataset-stats-vertical">
									<div className="stat-item-vertical">
										<span className="stat-number">2,575</span>
										<span className="stat-label">Total Sentences</span>
										<span className="stat-detail">
											Human-validated across all categories
										</span>
									</div>
									<div className="stat-item-vertical">
										<span className="stat-number">119</span>
										<span className="stat-label">Unique Contexts</span>
									</div>
									<div className="stat-item-vertical">
										<span className="stat-number">1,390</span>
										<span className="stat-label">Stereotypical</span>
									</div>
									<div className="stat-item-vertical">
										<span className="stat-number">1,185</span>
										<span className="stat-label">Anti-stereotypical</span>
									</div>
								</div>

								{/* Category Distribution - Right Side */}
								<div className="category-distribution">
									<h3>Distribution by Bias Category</h3>
									<div className="distribution-chart">
										<BiasDistributionChart />
									</div>
								</div>
							</div>{" "}
							{/* Dataset Example */}
							<div className="dataset-example">
								<div className="section-header">
									<BookOpen size={28} className="section-icon" />
									<h3>Example from Dataset</h3>
								</div>
								<div className="example-container">
									<div className="example-context">
										<strong>Context:</strong> Caste-based housing stereotypes
									</div>
									<div className="example-pairs">
										<div className="example-item stereotype">
											<div className="example-label">
												<span className="label-dot"></span>
												Stereotype
											</div>
											<div className="example-text">
												"The Dalit family lived in a small, cramped hut."
											</div>
										</div>
										<div className="example-item anti-stereotype">
											<div className="example-label">
												<span className="label-dot"></span>
												Anti-stereotype
											</div>
											<div className="example-text">
												"The Dalit family lived in a luxurious mansion."
											</div>
										</div>
									</div>
								</div>
							</div>
							{/* Access Information */}
							<div className="access-info">
								<div className="section-header">
									<Github size={28} className="section-icon" />
									<h3>Access Dataset & Code</h3>
								</div>
								<p className="access-description">
									The <strong>IndiCASA dataset</strong> and{" "}
									<strong>evaluation framework</strong> are available for{" "}
									<strong>research purposes</strong> under appropriate ethical
									guidelines. Access includes complete dataset, evaluation
									scripts, and documentation for academic and research use.
								</p>
								<div className="access-buttons">
									<a
										href="https://github.com/cerai-iitm/IndiCASA"
										target="_blank"
										rel="noopener noreferrer"
										className="btn btn-primary">
										<Github size={20} />
										GitHub Repository
									</a>
									<button
										onClick={handleCopyBibtex}
										className={`btn btn-secondary ${
											showCitationCopied ? "copy-success" : ""
										}`}>
										<Copy size={20} />
										{showCitationCopied ? "Citation Copied!" : "Copy Citation"}
									</button>
								</div>
							</div>
						</div>
					</div>
				</section>

				{/* Footer */}
				<footer className="footer">
					<div className="container">
						<div className="footer-content">
							<div className="footer-section main">
								<h3>IndiCASA Research</h3>
								<p>
									Advancing AI fairness through culturally-aware bias evaluation
									in Indian contexts
								</p>
								<div className="research-links">
									<a
										href="https://github.com/cerai-iitm/IndiCASA"
										target="_blank"
										rel="noopener noreferrer">
										<Database size={20} className="link-icon" /> Code & Dataset
									</a>
									<a
										href="https://arxiv.org/abs/2510.02742"
										target="_blank"
										rel="noopener noreferrer"
										className="research-link">
										<FileText size={20} className="link-icon" /> Research Paper
									</a>
								</div>
							</div>
							<div className="footer-section">
								<h4>Research Institutions</h4>
								<p>
									<a
										href="https://cerai.iitm.ac.in/"
										target="_blank"
										rel="noopener noreferrer"
										style={{ color: "inherit", textDecoration: "none" }}>
										Centre for Responsible AI
									</a>
									,{" "}
									<a
										href="https://www.iitm.ac.in/"
										target="_blank"
										rel="noopener noreferrer"
										style={{ color: "inherit", textDecoration: "none" }}>
										Indian Institute of Technology Madras
									</a>
								</p>
								<p>
									<a
										href="https://www.utdallas.edu/"
										target="_blank"
										rel="noopener noreferrer"
										style={{ color: "inherit", textDecoration: "none" }}>
										The University of Texas at Dallas
									</a>
								</p>
							</div>{" "}
							<div className="footer-section">
								<h4>Contact & Citation</h4>
								<p>For research inquiries and collaboration opportunities</p>
								<button onClick={handleCopyBibtex} className="citation-btn">
									<Copy size={16} /> Copy BibTeX Citation
								</button>
							</div>
						</div>

						<div className="footer-bottom">
							<p>
								&copy; 2025 IndiCASA Research Team. This work is intended for
								academic research and educational purposes.
							</p>
							<p>
								<strong>Content Warning:</strong> This research contains
								examples of harmful stereotypes for academic analysis.
							</p>
						</div>
					</div>
				</footer>
			</div>
		</MathJaxContext>
	);
}

export default App;
